{"pages":[{"url":"/pages/about-us.html","text":"果壳网平台组，致力于为全站各项目提供更为健壮和全面的基础平台服务。 部分开源项目： https://github.com/guokr","tags":"Python","title":"关于我们"},{"url":"/pattern-matching-problem.html","text":"摘要 在给定文本中排查一个固定字符串并不是一件很难的事情，但是倘若给定的字符串库十分庞大，以及涉及到多种字符串组合的时候，还能否高效迅速准确地排查就成了一个有挑战的问题。本文对字符串匹配问题做了一个较为系统深入的研究工作，并将其用于对果壳网现有敏感词项目的改进。 背景问题 敏感词的排查是所有社交网络都会面临的一个任务，对于固定的字符串，直观的想法可以是从列表中挨个取出所有的敏感词，再将它们与全文做一一比对即可，这样的思路虽然可行，但是在面对不断扩大的敏感词列表，既丑陋又笨拙。除此之外，线上还有一些动名词组合的情况需要考虑，即动词和名词分别可以出现，但是当它们以一种可以被识别的形式出现，并传达正确意义的时候，那就理应得到屏蔽。 对于固定的字符串，如何最快速地将它们全部抓出来；对于动名词组合，如何在做到不放过一条漏网之鱼的情况下尽量避免误伤，都是亟待解决的问题。 对于固定的敏感词匹配，可以抽象为字符串匹配问题，下面我们先对这个问题进行深入研究。 对问题的重定义 一个字符串是一个定义在有限字母集合 \\(\\sum\\) 上的字符序列。例如 \\(ABCDABC\\) 就是字母表 \\(\\sum = \\{A,B,C,D\\}\\) 上的一个字符串。那么字符串匹配问题实际上就是在一个大的字符串 \\(T\\) 中搜索某个特定的字符串 \\(p\\) 出现的位置。为方便表述，在这里我们不妨把 \\(T\\) 称作文本， \\(p\\) 称作模式串，长度为 \\(m\\) ，以及 \\(T\\) 和 \\(p\\) 都定义在同一个字母表 \\(\\sum\\) 上。 对于单个的字符串问题，根据搜索模式串的方式的不同，一般来说可以归结为三种思路： 基于前缀 从文本中挨个读字符，每读一个字符就更新相应的变量，检查是否存在一个可能的匹配。这种思路中最为典型的就是大名鼎鼎算法 KMP 以及后来居上的 Shift-Or 。 基于后缀 先设定一个滑动的窗口 \\(W\\) ，滑动窗口沿着文本 \\(T\\) 移动，对于任意位置上的窗口，在窗口中从后向前搜索窗口中的文本和模式串 \\(p\\) 的公共后缀。 BM 算法 就是使用了这种思想，但是一般情况下，BM 的一个简化版本 Horspool 性能更好，同时也有着非常广泛的应用，通常我们的浏览器、编辑器中的搜索功能都是它的实现。 基于子串 和第二种方法类似，这里也使用了滑动窗口的概念，也是从后向前搜索。有区别的地方是，搜索目标变成了是窗口中文本的最长后缀，显然它同时也是模式串 \\(p\\) 的一个子串。最早使用这个思想的算法是 BDM ，如果模式串 \\(p\\) 稍短，可以将它改进为 BNDM 。 鉴于基于前缀的搜索比较慢，以及基于子串的搜索方法只适合非常长的字符串，而这与我们的敏感词匹配要求并不复合，因此我们选择基于后缀的搜索方法。 前面提到这种基于后缀的搜索算法的基本思路是要设定一个滑动的窗口，然后让它沿着文本移动进行匹配，事实上这种方法最大的难点也就在于如何安全地移动窗口，以避免错过正确的匹配。 Boyer-Moore 算法 我们先设定三个移动窗口函数 \\(d_1,d_2,d_3\\) 来用来分别对应后面提到的移动窗口时候面对的三种不同的情况。这里我们不妨假设已经读入了一个既是窗口中文本的后缀，也是模式串 \\(p\\) 的子串的字符串 \\(p_0\\) ，以及恰好下面即将读入的文本中的字符串 \\(\\alpha\\) 与 \\(p\\) 中的下一个字符 \\(\\beta\\) 不一样。 我们先来看第一种情况，后缀 \\(p_0\\) 在模式串 \\(p\\) 的多个位置有出现，假设最右出现的位置为 \\(j\\) ，这里不包括在模式串末尾的情况。此时我们已经匹配好的后缀 \\(p_0 = p_{j-|p_0|+1}…p_j\\) ，以及安全的移动方法是，将窗口往右移动 \\(m-j\\) 个字符，才能使得文本中的 \\(p_0\\) 与 \\(p\\) 下一个 \\(p_0\\) 的出现的位置对齐。对于 \\(p\\) 的每一个后缀，都需要计算它到它下一次出现位置之间的距离，这个距离我们就用前面定义的窗口移动函数 \\(d_1\\) 来表示。显然的一个特例是，如果 \\(p_0\\) 在 \\(p\\) 中只出现一次，那么此时 \\(d_1(p_0) = m\\) 。 再说第二种情况，此时后缀 \\(p_0\\) 不出现 \\(p\\) 中的其他位置。此时考虑情况如下图，即 \\(p_0\\) 的一个后缀 \\(p_0'\\) 同时也是 \\(p\\) 的一个前缀。显然，在这种情况下，如果直接移动整个窗口的大小是很危险的动作，可能会错过正确的匹配。于是我们需要对模式串的所有后缀计算第二个函数 \\(d_2\\) ，即对于 \\(p\\) 的每一个后缀 \\(p_0, p_1,…p_i,…,p_n\\) 来说， \\(p_i\\) 会存在一个 \\(p_i'\\) ，它是 \\(p\\) 的前缀和 \\(p_i\\) 的后缀的重合体中长度最长的那个。 最后再来说存在的第三种情况，当我们的搜索窗口从后向前移动时，在文本字符 \\(\\alpha\\) 处不能成功匹配。此时如果用第一种情况下的 \\(d_1\\) 进行窗口移动，并且 \\(p\\) 中对应的字符并不是 \\(\\alpha\\) ，而是 \\(\\beta\\) ，那么将会对新的搜索窗口多一次验证，这显然是冗余的。我们的第三种移动函数 \\(d_3\\) 也就应运而生了。 \\(d_3\\) 可以用来保证下一次验证时文本字符 \\(\\alpha\\) 与 \\(p\\) 中的 \\(\\alpha\\) 对齐。我们不妨将 \\(d_3(\\alpha)\\) 记作 \\(\\alpha\\) 在 \\(p\\) 中最右的位置到末尾的距离。当然，倘若 \\(\\alpha\\) 在 \\(p\\) 中并没有出现，那么 \\(d_3(\\alpha) = m\\) 。 有了这三种情况的拆分，BM 算法整体就相对好理解了。回到开始，即将读入的文本中的字符串 \\(\\alpha\\) 与 \\(p\\) 中的下一个字符 \\(\\beta\\) 不一样，窗口移动的距离就由这三个函数来决定： 比较 \\(d_1(p_0)\\) 与 \\(d_2(p_0')\\) ，将它们中较大的那一个用作窗口移动的距离，因为我们期望的是 \\(p_0\\) 和它在 \\(p\\) 中下一次出现的位置对齐； 取上面的结果，并与 \\(m-d_2(p_0)\\) 来作比较，将它们中较小的用作窗口移动的距离，这是很显然的，因为 \\(m-d_2(p_0)\\) 已经是最大可以接受的安全距离。 BM 的搜索时间复杂是 \\(O(mn)\\) ，总的来说还算不错，但是比较麻烦的地方在于需要计算 \\(d_1, d_2, d_3\\) ，下面介绍它的一个简化版本，也是它应用最多的一个版本，Horspool。 Horspool 算法 Horspool 相对于 BM 最大的简化的地方是使用了一个统计上的经验，即，在面对字母表的 \\(\\sum\\) 较大的时候， \\(d_3\\) 总是有很大概率产生最大的移动距离。基于此，我们不妨就让 \\(d_3\\) 能够产生更大的移动距离。 不妨用下图来说明，对于每个移动的搜索窗口，Horspool 将窗口中文本字符串 \\(T\\) 的最后一个字符（图中的 \\(\\theta\\) ）和模式串 \\(p\\) 的最后一个字符 \\(p[-1]\\) 进行比较。如果它们相等，则需要进行验证。过程是首先在搜索窗口中从后向前对 \\(T\\) 和 \\(p\\) 进行比较，直到出现完全相等（匹配成功），或者第一个不匹配（图中 \\(\\alpha\\) 和 \\(\\beta\\) ），然后，无论是否成功，窗口的下一个移动距离是依据 \\(p\\) 中 \\(\\theta\\) 的位置： 下面对算法整体的流程简述： 输入 ：模式串 \\(p = p_1p_2…p_m\\) ；目标文本 \\(T = t_1t_2…t_n\\) ； 输出 ：窗口指针位置 \\(pos\\) ； 步骤 ： 数据预处理阶段，主要是做一些存储。对所有 \\(c \\in \\sum\\) ，有 \\(d[c] \\leftarrow m\\) ；对所有 \\(j \\in 1,2,…,m-1\\) ，有 \\(d[p_j] \\leftarrow m-j\\) ； 搜索阶段。首先将当前位置 \\(pos\\) 置为0，检查 \\(pos \\leqslant n-m\\) ，倘若是，那么 \\(j \\leftarrow m\\) ；然后去 3，如果否，跳转到 5； 检查 \\(j>0\\) 且 \\( t_{pos} == p_j\\) ，倘若是，有 \\(j \\leftarrow j-1\\) ，然后去 4；如果否，跳转到 5； 检查如果 \\(j == 0\\) ，然后记录下匹配位置 \\(pos+1\\) ，跳转到 5； 窗口位置移动，即当前位置的指针执行移动： \\(pos \\leftarrow pos+d[t_{pos+m}]\\) ，后跳转 2； 多字符串匹配 回到开头，我们的问题其实是在一个给定的敏感词列表中进行排查，即表中的所有字符串都不能在文本中出现，即 字符串集匹配 ，或者 多字符串匹配 问题。 其实单字符出匹配问题可以很自然地扩展到这样的多字符串匹配问题， \\(P = \\{p_1,p_2,…,p_r\\}\\) 就是一组我们要搜索的字符串。在系统分析这个问题之前，我们先不妨做一些定义： \\(p_i\\) 是定义在一个有限字母表 \\(\\sum\\) 上的字符串， \\(p_i=p_{i1}p_{i2}...p_{im_i}\\) 。记 \\(|P|\\) 为 \\(P\\) 中所有字符串长度之和，即 \\(|P| = \\sum_{i=1}&#94;{r}|p&#94;i|=\\sum_{i=1}&#94;{r}m_i\\) ，同时 \\(l_{min}\\) 和 \\(l_{max}\\) 分别是 \\(P\\) 中最短和最长模式串的长度，目标文本 \\(T = t_1t_2…t_n\\) 。 注意 \\(P\\) 中的少数字符串可能是集合中其他字符串的前后缀，例如 涉枪涉爆 类别中既有「枪弩」也有「气枪弩」，也就是说如果「枪弩」被匹配成功，显然「气枪弩」也是会被匹配出来。因此，模式串总的出现次数最大可能为 \\(rn\\) 。那么我们真正的目标实际上是所有满足 \\(p_i=t_{j-|p_i|+1}…t_j\\) 的整数对 \\((i,j)\\) 。 其实对于多字符串匹配的一个最简单的思路就是使用前面的单字符串匹配方法，遍历整个集合即可。但是这样会直接搜索的时间复杂度为 \\(O(rn)\\) ，以及还有数据预处理阶段的大量时间。显然这么做是笨拙的，也是低效的，我们下面对前面使用的单字符串匹配算法进行一定的扩展，以达到性能上的优化。 虽然单字符串匹配问题中的三种思路都是可以扩展，但是对于我们的问题后缀搜索效率最高，故将我们的主要工作重心放在基于后缀的字符串匹配问题的扩展上。 Trie Tree 我们先介绍一种称之为前缀树的数据结构，或者叫字典树，trie tree，之所以要先介绍它首先是因为它能够帮助理解后面的推导与演算，还有一个重要的原因是前缀树在非常多的字符串问题中扮演了极其重要的角色。 那我们就这个项目举例来看，假设集合 \\(P = \\{p_1,p_2,…,p_r\\}\\) 对应的前缀树结构其实是一棵有向的有根树，每个从根节点开始到叶子节点结束的路径上的所有标号构成的字符串，都对应着集合 \\(P\\) 中的某一个元素 \\(p_i\\) ，反过来，集合 \\(P\\) 中的每一个元素也都对应着前缀树中的一条从 root 到叶子节点的路径。对于前缀树来说，倘若一个结点 \\(q\\) 对应这一个 \\(P\\) 中的某个元素，那我们我们就将它称之为可以接受的状态，函数 \\(F(q)\\) 包含了 \\(q\\) 所对应的集合 \\(P\\) 中的所有字符串。 下图就是一个简单例子，是 \\(P = \\{tee, tea, teacher\\}\\) 所对应的 trie 结构。 自动机 在计算机科学中，自动机这个词其实意义很多。我们下面单就字符串匹配领域来说说自动机，照例，我们先给出一些定义。 一个有限状态自动机 \\(\\mathcal{A}\\) ，或者简称做自动机，一般是由： 一个有限的状态集合 \\(\\mathcal{Q}\\) 一个初始状态 \\(\\mathcal{s} \\in \\mathcal{Q}\\) 一个输入字母表 \\(\\sum\\) （非空有限的状态集合） 一个可以被接受的状态的集合 \\(\\mathcal{F} \\subseteq \\mathcal{Q}\\) 一个状态转移函数 \\(\\delta: \\mathcal{Q} \\times \\sum \\rightarrow \\mathcal{Q}（例如：\\delta (q,\\sigma) = p,(p,q \\in \\mathcal{Q}, \\sigma\\in\\sum）\\) 这样，自动机 \\(\\mathcal{A} = (\\mathcal{Q}, \\sum, \\mathcal{s}, \\mathcal{F}, \\delta)\\) 就用这样的五元组来表示。 在实际应用中，根据状态转移函数的形式大抵可以分为两类：一类称之为非确定的有限自动机，它的状态转移函数 \\(\\delta\\) 将某一个状态 \\(q\\) 通过一个给定字符 \\(\\sigma\\) 关联到多于1个的状态，即 \\(\\delta(q,\\sigma) = \\{q_1,q_2,…,q_k\\}, k>1\\) ，或者某些状态装一能被标记为 \\(\\varepsilon\\) 。此时，我们的状态转移函数 \\(\\sigma\\) 就可以用三元组 \\(\\Delta = \\{(q,\\sigma,q')\\}, q\\in\\mathcal{Q}, \\sigma\\in\\sum， q' \\in \\delta(q,\\sigma)\\}\\) 的集合来表示。另外一类则称之为确定的有限自动机，区别在于它的状态转移函数 \\(\\delta\\) 可以用一个部分函数 \\(\\delta：\\mathcal{Q}\\times\\sum \\rightarrow \\mathcal{Q}\\) 来表示，如果 \\(\\delta(q,\\sigma) = \\{q'\\}\\) ，那么 \\(\\delta(q,\\sigma) = q'\\) ，下图给出了两个例子： 上图中给出了两种自动机的模型。其中 \\(0\\) 是初始状态，灰色表示可以被接受的终止状态。左边的是非确定性的自动机，因为从状态 \\(0\\) 通过 \\(d\\) 或者 \\(d\\rightarrow a\\rightarrow b\\) 到达状态 \\(6\\) ，即便是只通过 \\(d\\) 的话，也可以达到 \\(6\\) 或者 \\(2\\) 。而右图则是确定性的自动机，因为对于任何一个字符，每个状态都只能转移到一个状态。 那么在这个自动机 \\(\\mathcal{A} = (\\mathcal{Q}, \\sum, \\mathcal{s}, \\mathcal{F}, \\delta)\\) 中，如果将从状态 \\(s\\) 到一个可以被接受的状态路径上的标记连起来可以得到的字符串则可以被自动机 \\(\\mathcal{A}\\) 识别。 注意，在非确定的有限状态机中，是允许状态转移上被标记为空串的，我们将这样的状态转移称之为 \\(\\varepsilon\\) -转移。这就意味着不必要读入一个字符也可完成一次状态转移。那么如果到达了一个 \\(\\varepsilon\\) 转移的原状态，那么不用读取就可以立即完成跳转，这样也是可以看做是读入了一个空串。事实上， \\(\\varepsilon\\) 转移通常用来简化这种非确定的有限状态自己懂的构造，但是总是存在一个与之等价的不含 \\(\\varepsilon\\) 转移的自动机。 但是总而言之，不管是在确定性还是非确定性的有限状态自动机中，如果将从 \\(s\\) 到某一个状态的 \\(s_0\\) 的路径上标记穿起来可以得到字符串 \\(x\\) ，那么就说，读入 \\(x\\) 后的状态 \\(s_0\\) 是一个活动状态。在任何时候，确定性的有限状态自动机中，最多只有一个活动状态，而非确定性的则可能存在多个。 上图表示两个自动机中的状态转移不会构成环，这样的自动机不论确定性与否，都被称之为无环的下图两个的自动机则是有环的，一个有环的自动机可以接受的状态集合可能是无限的。 例如上图中的有环自动机，左图可以识别 \\(P=\\{dab\\}\\) , 也可以识别 \\(P=\\{dab, dabaab, dabaabaab,…\\}\\) 。 基于后缀的多字符串匹配 穿插着了解了前缀树和自动机之后，我们回到上文继续讨论基于后缀的多字符串匹配问题，我们这里主要的思想是对使用后缀匹配思想单字符串的匹配算法的扩展。这样的第一个算法对 BM 的扩展， Commentz-Walter，当然也有对 Hospool 的扩展，下面我们逐一讨论。 Commentz-Walter Commentz-Walter（以下简称 CW 算法） 算法是对 BM 算法的扩展，它的应用非常广泛，例如 UNIX 下的非常著名的搜索程序 Grep 的第二个版本就是它的一个实现。 CW 算法中用前缀树来表示 \\(P=\\{p_1,p_2,…,p_r\\}\\) 的反转 \\(P_{rv}=\\{p_{1,rv},p_{2,rv},…,p_{r,rv}\\}\\) ，并用它来识别文本字符。由于这个算法性能差强人意，我们简单说说思想。在窗口移动过程中，指针 \\(pos\\) 指向 \\(l_{min}\\) ，对于 \\(pos\\) 的每个新位置，从 \\(pos\\) 开始，从后向前识别文本 \\(t_1t_2…t_{pos}\\) 的最长后缀 \\(u\\) ，使得 \\(u\\) 也是某一个模式串的后缀，然后根据在多模式串集合上扩展的 BM 算法的三个函数 \\(d_1,d_2\\) 和 \\(d_3\\) ，将 \\(pos\\) 右移。以及对于前缀树的每个状态，都需要计算 \\(d_1\\) 和 \\(d_2\\) ，当最长后缀 \\(u\\) 在可以接受的状态集合中被识别出来并抵达状态 \\(q\\) 时，再根据 \\(d_1\\) 和 \\(d_2\\) 进行移动。 \\(d_1(q)\\) 是使得 \\(u = \\mathcal{L}(q)\\) 与某一个模式串的子串对齐的最小移动距离 \\(p_j \\in \\mathcal{P}\\) \\(d_2(q)\\) 是使得 \\(u = \\mathcal{L}(q)\\) 的一个后缀与某一个模式串的前缀相匹配的最小移动距离 \\(p_j \\in \\mathcal{P}\\) 那么对于 \\(\\sum\\) 中每个字符 \\(\\alpha\\) 和每一个位置 \\(0\\leqslant k<l_{max}\\) ， \\(d_3[\\alpha,k]\\) 是使得位置 \\(pos-k\\) 处的字符串与模式串 \\(p\\) 中某个字符相匹配的最小移动距离 \\(p_j \\in \\mathcal{P}\\) 。 下面简述如何用这三个函数来计算我们所需的移动距离，我们不妨假设从文本位置 \\(pos\\) 从后向前读入了 \\(k\\) 个字符，并且在自动机中抵达了状态 \\(q\\) ，那么移动距离 \\(s[q,pos,k]\\) 可以由下面的公式推导而来： $$s[q,pos,k] = min\\left\\{\\begin{matrix} max(d_1[q],d_3[t_{pos-k},k])\\\\ d_2[q] \\end{matrix}\\right.$$ 上式是 BM 算法窗口移动的直接扩展，显然有 \\(d_2\\leqslant l_{min}\\) ，所以最长的跳跃距离必然不会超过 \\(l_{min}\\) 。 CW 算法的时间复杂度为 \\(O(n \\times l_{max})\\) ， \\(d_1,d_2,d_3\\) 可以在 \\(O(|P|)\\) 的时间内计算完成。 Set Horspool Set Horspool （下称 SH）是对单字符串的 Horspool 算法扩展，它也可以看作是 CW 算法的简化。 SH 算法的基本流程如图，假设当前文本位置 \\(pos\\) 初始化为 \\(l_{min}\\) ，从 \\(pos\\) 开始，从后向前读入文本字符。这里使用所有模式串反转 \\(p_{rv}\\) 构建的前缀树来完成识别过程，如果到达状态 \\(s_0\\) 恰好属于可以接受的结束状态，就标记下一个匹配，假设当无法继续识别读入的文本字符，就根据第一个读入的文本字符 \\(\\beta\\) 来决定 \\(pos\\) 的移动。 \\(pos\\) 移动到使得 \\(\\beta\\) 与它在前缀树中下一次出现位置对齐的地方。一个显然的特例是，如果 \\(\\beta\\) 不再重复出现，那么就直接移动 \\(l_{min}\\) 即可。 下面简述 SH 算法的流程： 输入 ：模式串集 \\(P = \\{p_1,p_2,…,p_r\\}\\) ，目标文本 \\(T=t_1t_2…t_n\\) ； 输出 ：窗口指针位置 \\(pos\\) ； 步骤 ： 预处理阶段。 \\(HO \\leftarrow Trie(P_{rv}=\\{p_{1,rv},p_{2,rv},…,p_{r,rv}\\})\\) ，其中这里的 \\(\\delta_{HO}\\) 是状态转移函数；对于 \\(c\\in\\sum\\) ，有 \\(d[c]\\leftarrow l_{min}\\) ；对于所有 \\(j\\in 1,2,…,r\\) ，有 $$d[p_{jk}]\\leftarrow min(d[p_{jk}], m_j-k), k\\in 1,2,…,m_j-1$$ 初始化当前指针位置， \\(pos \\leftarrow l_{min}\\) ，检查 \\(pos \\leqslant n\\) ，如果是， \\(j \\leftarrow 0\\) ，也即从 \\(HO\\) 中拿到它的初始状态，然后跳转到 3，如果不是，跳转到 6； 检查如果 \\(pos - j > 0\\) 且 \\(\\delta_{HO}(t_{pos-j}, Current)\\neq\\theta\\) ，跳转到 4，如果不是，跳转到 6； 检查 \\(Current\\) 是否是可以被接受的终止状态，如果是则记录下 \\(\\mathcal{F}(Current, pos)\\) ，如果不是，跳转到 5； 执行状态转移 \\(Current \\leftarrow \\delta_{HO}(t_{pos-j}, Current)\\) ，同时 \\(j \\leftarrow j+1\\) ； 位置指针转移 \\(pos \\leftarrow pos + d[t_{pos}]\\) ； SH 算法的复杂度是 \\(O(n\\times l_{max})\\) ，一般来说，它只适合模式串集合很小，但是字母表 \\(\\sum\\) 却很大的场合。 Wu-Manber SH 在多模式串的情况下其实性能很差，主要归咎于字母表 \\(\\sum\\) 中每个字符通常都以较高概率出现在某个模式串中，从而导致移动距离下降。 Wu-Manber（下称 WM）解决的思路是读入一块字符，从而降低字符块在某个模式串中出现的概率。这里我们不妨假设块的长度为 \\(B\\) 。问题的难点在于，如果 \\(B\\) 比较大，那么总共就可能会有 \\(|\\sum|&#94;B\\) 个不同的块，对存储空间是一个不小的挑战。 WM 首先使用一个散列函数 \\(h_1\\) 将所有可能的块散列到一个有限的表 \\(SHIFT\\) 上，两个不同的块有可能被散列映射到 \\(SHIFT\\) 的同一个位置。对于每个新的位置，如果读入一块字符 \\(Bl\\) 而不是像 Horspool 那样只读入最后一个字符，则必须确保 \\(Bl\\) 的移动距离 \\(SHIFT(h_1(Bl))\\) 是安全的。为了确保这一点，算法在 \\(SHIFT(j)\\) 中存入满足 \\(j=h_1(Bl)\\) 的所有块的移动距离的最小值。具体地说，构建 \\(SHIFT\\) 表的步骤如下： 如果块 \\(Bl\\) 不出现在 \\(P\\) 中的任何一个模式串中，则可以安全地向右移动 \\(l_{min}-B+1\\) 个字符。因此，将表的每一项都初始化为 \\(l_{min}-B+1\\) 。 反之，如果块 \\(Bl\\) 出现在 \\(P\\) 中的某一个模式串中，我们不妨把它记作 \\(p_i\\) ，则需要找出 \\(Bl\\) 在 \\(p_i\\) 中最右出现的末尾位置 \\(j\\) ，然后将 \\(SHIFT(h_1(Bl))\\) 置为 \\(m_i -j\\) 。为了计算 \\(SHIFT\\) 表的所有值，对于每个模式串 \\(p_i=p_{i1}p_{i2}…p_{im_i}\\) 的每个块 \\(B=p_{i,j-B+1}…p_{i,j}\\) ，都需要在 \\(SHIFT\\) 中寻找出对应的表项 \\(h_1(B)\\) ，并将 \\(SHIFT(h_1(Bl))\\) 置为 \\(m_i -j\\) 和它当前值中的较小的那个。 其实块长 \\(B\\) 取决于三个因素：最短的关键词长度 \\(l_{min}\\) ；模式串集合的大小以及字母表 \\(\\sum\\) 的大小。研究表明，取 \\(B = log_{|\\sum|}(2\\times l_{min} \\times r)\\) 能够产生最好的实验结果。 \\(SHIFT\\) 的大小也是随着可用空间的大小而变化。 只要移动距离 \\(l>0\\) ，就可以安全地将当前位置向右移动，当移动距离 \\(l = 0\\) 时，当前位置的左边可能就是一个被成功匹配的模式串，那么对于这种情况，WM 巧妙地使用另一个散列表 \\(HASH\\) 和散列函数 \\(h_2\\) ，它的每个表项 \\(HASH(j)\\) 本质是一个链表结构，记录了最后一个块在 \\(h_2\\) 的散列映射中到 \\(j\\) 的所有模式串集合。于是这样就可以顺藤摸瓜找出那些最后一块的散列值与当前读入的文本块 \\(Bl\\) 散列值相同的所有模式串。 搜索阶段和 SH 算法是类似的，首先将当前位置 \\(pos\\) 初始化置为 \\(l_{min}\\) ，那么对于每个当前位置 \\(pos\\) ，从后向前读入 \\(B\\) 个字符的块 \\(Bl\\) 。这时倘若 \\(j = SHIFT(h_1(Bl)) > 0\\) ，那么将窗口移动到位置 \\(pos+j\\) ，并继续搜索；反之，如果 \\(SHIFT(h_1(Bl)) = 0\\) ，那我们就用 \\(HASH\\) 算出文本块对应的一组模式串 \\(HASH(h_2(Bl))\\) ，并逐个与文本进行比较，算法流程如下： 输入 ：模式串集 \\(P = \\{p_1,p_2,…,p_r\\}\\) ，目标文本 \\(T=t_1t_2…t_n\\) ； 输出 ：窗口指针位置 \\(pos\\) ； 步骤 ： 预处理阶段，计算 \\(B\\) ，然后继续构建两个哈希表 \\(SHIFT\\) 和 \\(HASH\\) ； 初始化当前指针位置， \\(pos \\leftarrow l_{min}\\) ，检查 \\(pos \\leqslant n\\) ，如果是， \\(i \\leftarrow h_1(t_{pos-B+1}…t_{pos}\\) ，然后跳转到 3，否则算法结束； 检查 \\(SHIFT[i] == 0\\) ，如果是， \\(list \\leftarrow HASH[h_2(t_{pos-B+1}…t_{pos})]，这里的 $list\\) 装载了匹配好的模型，挨个对应着目标文本 \\(T\\) ，随后照例 \\(pos \\leftarrow pos + 1\\) ，如果不是，跳转到 4； 窗口指针位置转移 \\(pos \\leftarrow pos + SHIFT[i]\\) ； 以上就是对于固定的敏感词列表的排查算法的研究，其中也包含了大量的实践操作，虽然它们各有优劣，但是就目前敏感词列表大小，词组长度以及字母表的长度而言，Wu-Manber 是不二的选择。 下面我们继续介绍对于动名词组合的情况的思考与实践： 动名词组合 如本文开头陈述，我们的排查范围除了固定的敏感词列表之外，还有一类动名词组合同样需要处理。不妨举例，动词 购买 和名词 自制手枪 都是可以独立出现，但是他们不能以组合的形式出现，例如出现 购买自制手枪 和 购买一把自制手枪 都应该被识别出来，而如果两者分别出现的问题距离很远，就不应该被误伤。显然这里的难点在于指定两者之间的距离究竟为多少。 既然人工很难直接设定一个阈值来判定，我们不妨考虑机器学习思路，我们使用决策树算法来进行训练和分类预测，算法在 前文 有详述，不再重复，这里仅说明特征和思路。 对于训练集： $$D = \\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\\}$$ 这里向量 \\(x_i = [x_{1i},x_{2i}]\\) 考虑目前只考虑两个最显而易见的维度：整个文章的长度，以及出现的动名词之间的最短距离。同样是二分类问题，所以 \\(y\\) 将会在 \\(0,1\\) 中取值。 简单说明一下动名词之间的最短距离计算方法，依照前文的固定敏感词算法分别搜索 $$P_{action} = \\{p_1,p_2,…,p_n\\}, P_{noun} = \\{p_1,p_2,…,p_m\\}$$ 也就得到它们分别的位置列表： $$pos_{action} = [a_1,a_2,…,a_n], pos_{noun} = [n_1,n_2,…,n_m]$$ 鉴于此处的列表长度相对非常短， \\(O(n&#94;2)\\) 遍历的时间复杂度可以接受，故不再赘述。 总结 虽然排查敏感词从功能上来说并不是一个十分艰巨的任务，但是我们还是对这个问题做了一个较为系统和深入的研究工作，以期从算法角度得到高效且优雅的解决方案。其中对于动名词组合的情形，我们采取决策树算法，就文本的总长度和动名词之间的最短距离作为输入特征，训练得到两者之间的触发距离。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"算法团队","title":"敏感词项目中的柔性字符串匹配问题"},{"url":"/decision-tree-in-anti-spam.html","text":"摘要 决策树（Decision Tree）是机器学习领域中一种基本的分类与回归方法，在分类的问题中，表示基于特征对实例进行分类的过程。粗浅地看，可以认为是一组特定 if-else 规则的集合，也可以认为是定义在特征空间和类空间上的条件概率分布。由于算法既简单易懂，又十分高效，所以在实际应用中非常广泛。本文主要介绍决策树算法在果壳网反垃圾系统中的应用与实践心得。 背景问题 果壳网社区项目是目前果壳的主要流量来源，包含有果壳小组，果壳问答，知性社区，MOOC 社区等。社区内部有大量的垃圾用户，频繁发帖，给运营人员的工作带来了很大的压力，也影响了社区正常的环境。 如何区分正常用户和 spammer 可以看成是一个分类问题，准确地说，其实是一个二分类问题。常用的分类机器学习算法都可以使用，包括但不限于SVM，逻辑回归，朴素贝叶斯。由于这些分类算法都属于有监督的机器学习，因此必须要使用一些已经标注好的样本进行训练。毫无疑问，现有的用户信息都在 log 中，我们究竟要取其中的哪些作为用户的 feature 来训练，以及是否还需要人工标注，都是接下来需要面临的特征工程问题。 对 raw data 的清洗和挖掘是整个机器学习系统的第一个关键性步骤，对算法的设计和结果都会产生重大的影响，会另写一篇详述，在此不表。 不妨假设我们得到的训练样本集合： $$D=\\{(x_1,y_1), (x_2,y_2),…,(x_N,y_N)\\}$$ 这里， \\(x_i\\) 是一个 \\(m\\) 维的向量， \\(x_i=[x_{1i},x_{2i},…,x_{mi}]\\) ，因为是二分类问题，所以 \\(y\\) 将会是在 \\({0,1}\\) 中取值。因此问题转化为我们需要寻找一个模型，而它恰恰是从训练 \\(D\\) 中样本 \\((x_i,y_i)\\) 所带来的信息学习而来。具体地说，对于一个样本输入 \\(x_i\\) 来说，我们的模型 \\(y=f(x)\\) 会给出一个输出 \\(f(x_i)\\) ，而训练数据集中对应的输出是 \\(y_i\\) ，如果这个模型有很好的预测能力，训练样本的输出 \\(y_i\\) 应该与模型的输出 \\(f(x_i)\\) 一致。 在此处的二分类问题中，我们的损失函数是0-1损失，所以测试误差就变成了常见的测试数据集上的误差率: $$e_{test}=\\frac{1}{N'}\\sum_{i=1}&#94;{N'}I(y_i\\neq f(x_i))$$ 这里 \\(I\\) 是指示函数，即当 \\(y_i\\neq f(x_i)\\) 的时候为1，否则为0。 特征启发 虽然特征工程不属于本文范畴，但关于究竟哪些用户数据可以用来判定 spammer，不妨一谈。在果壳网现有的几个社区型项目中，既有「写日志」，「回答问题」，「小组发帖」，「小组回复」这样的高投入行为，也有像「点赞」，「推荐」这样的低投入行为，那么用户在这些行为中投入所期待的回报是什么呢？我们认为，这里的回报可以等价于该用户所带来的社交影响力。具体表现为： spammer 的很多动作会在动态列表中显示，进而出现在其所有 follower 的 timeline 中； spammer 发的帖子会被小组的所有成员看到和点击； 那么显然，投入和回报的比例在普通用户和 spammer 身上是有理由区分的。通过实际数据调研同样发现： 多数 spammer 更倾向于在人数较多的小组发广告； 日志较长，状态较多，且没有 follower，或极少； 所有 spammer 都未使用不会出现在他人 timeline 中的功能，例如「添加到果篮」； 据此，我们将 user_profile 中可能涉及社交回报的数据筛选出来作为训练样本的特征，然而再进行进一步的特征工程工作。 在拥有这样的启发思路之后，我们发现决策树算法很符合从行为上判断 spammer 的逻辑，同时分类结果也具备很强的可解释性，考虑到特征中有很多连续变量，我们使用 C4.5决策树算法。 决策树学习 背景部分介绍的0-1损失函数，实际上也就是正则化的极大似然函数，换言之，决策树的学习就等价于以损失函数为目标函数的最小化。那么当损失函数确定下来之后，我们的问题就转变成在损失函数意义下选择最好的决策树。 整个算法流程实际上是递归地寻找最优（最具有区分度）特征，并以此来渐渐分特征空间，也进而对应着决策树的构建过程。具体来说： 选择一个最具有区分度的特征，分割特征空间为子集； 根据分割结果判断是否所有样本均被正确分类； 如果是，则标记叶子结点，结束；如果不是，对划分的子集选择新的最优特征重复 1 的操作，直到全部分类正确或没有合适的特征为止； 那么对于步骤 1，我们应该如何比较这些特征的区分度呢？我们使用信息论中的熵（entropy） 概念来作为判定的基础。 在信息论中，熵是表示随机变量不确定性的度量，假设 \\(X\\) 是一个取有上限值的离散随机变量，那么其概率分布为： $$P(X=x_i)=p_i, i=1,2,…,n$$ 则随机变量 \\(X\\) 的熵定义为： $$H(X)=-\\sum_{i=1}&#94;{n}p_ilog(p_i)$$ 那么显然，熵越大，随机变量的不确定性也就越大。当随机变量为 \\({0,1}\\) 时，即 \\(X\\) 的分布为： $$P(X=1)=p, P(X=0)=1-p, 0\\leq p \\leq 1$$ 那么对于两个变量 \\((X,Y)\\) ，它们的联合概率分布为： $$P(X=x_i, Y=y_i) = p_{ij}, i=1,2,…,n; j = 1,2,…,m$$ 条件熵 \\(H(Y|X)\\) 表示的是已知随机变量 \\(X\\) 的条件下随机变量 \\(Y\\) 的不确定性，准确定义为 \\(X\\) 给定条件下 \\(Y\\) 的条件概率分别的熵对 \\(X\\) 的数学期望： $$H(Y|X)=\\sum_{i=1}&#94;{n}p_iH(Y|X=x_i), p_i=P(X=x_i), i=1,2,…,n$$ 在给出了不确定性的度量之后，我们还需要给出表示一种「得知特征 \\(X\\) 的信息而使得类 \\(Y\\) 的信息的不确定性减少的程度」，称之为 信息增益 。 最简单的信息增益的算法是将集合的经验熵与给定某条件的情况下的经验条件熵直接取差值，例如定义特征 \\(A\\) 对数据集 \\(D\\) 的信息增益 \\(g(D,A)\\) 可以表示为： $$g(D,A)=H(D)-H(D|A)$$ 即表示「由于特征 \\(A\\) 而使得对数据集 \\(D\\) 的分类的不确定性减少的程度」，虽然这样的做差算法是合理的，但是会偏向于选择取值较多的特征的问题，因此我们使用 信息增益比 来对它进行校正。 那么特征 \\(A\\) 对数据集 \\(D\\) 的信息增益比 \\(g_R(D,A)\\) 定义为其信息增益 \\(g(D,A)\\) 与训练数据集 \\(D\\) 关于特征 \\(A\\) 的值的熵 \\(H_A(D)\\) 之比，即 $$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$$ 其中， $$H_A(D)=-\\sum_{i=1}&#94;{n}\\frac{|D_i|}{|D|}log\\frac{|D_i|}{|D|}，n 是特征 A 取值的个数$$ C4.5决策树学习流程如下： 输入 : 训练数据集： \\(D\\) ；特征集合： \\(F\\) ；阈值： \\(\\varepsilon\\) ； 输出 ：一棵完整的决策树模型： \\(T\\) ； 步骤 ： 如果 \\(D\\) 中所有的样本都属于同一个类 \\(C_k\\) ，则将 \\(T\\) 置为单节点树，以及将 \\(C_k\\) 作为该结点的类，返回 \\(T\\) 为输出； 如果 \\(A = \\phi\\) ，即供判断的特征集合为空，则将 \\(T\\) 置为单节点树，并将 \\(D\\) 中样本数量最大的类 \\(C_k\\) 作为该结点的类，返回 \\(T\\) 为输出； 如果1和2都不满足，按照 信息增益比 对 \\(A\\) 集合中所有元素对 \\(D\\) 进行计算，选择当前其中最大的特征 \\(A_g\\) ； 如果 \\(A_g < \\varepsilon\\) ，则将 \\(T\\) 置为单节点树，并将 \\(D\\) 中样本数量最大的类 \\(C_k\\) 作为该结点的类，返回 \\(T\\) 为输出； 反之如果 \\(A_g \\geqslant \\varepsilon\\) ，对 \\(A_g\\) 统计的取值范围中的每一个可能的值 \\(a_i\\) ，依照 \\(A_g = a_i\\) 将 \\(D\\) 分割成若干个非空子集 \\(D_i\\) ，将 \\(D_i\\) 中样本数量最大的类作为标记，构建下属子节点，并由此节点和子节点（递归）构成 \\(T\\) ，最终返回 \\(T\\) 为输出； 对任意结点 \\(i\\) ，都以 \\(D_i\\) 为训练数据集，并以 \\(A-{A_g}\\) 为特征集，递归调用上述5个步骤，得到子树 \\(T_i\\) ，返回 \\(T_i\\) ，算法结束。 此时得到的决策树已经能很好地对果壳现有的用户进行分类，根据用户的社交活动属性将大多数 spammer 从中分离出来，但是我们发现虽然准确率非常高，但是仅仅存在于训练数据集，在测试集上表现平庸，甚至很差。这是因为在决策树训练步骤中，模型是通过不断地递归直到不能继续为止。那么这样产生的树极有可能会出现过拟合，原因就在于学习时过多地考虑如何对训练数据进行正确地分类，从而导致构造的决策树过于复杂。最常用的解决办法是对决策树深度，或者说是它的复杂度进行限制，具体地，就是对决策树进行剪枝操作。 剪枝就是从已经生成的决策树上删除一些子树或者结点，然后将它的父节点当做新的叶子结点，从而使整个决策树模型变得简单，也具有更强的泛化能力和容错能力。 决策树的剪枝方法有很多，我们使用一种较为简单也较为常用的方法。 决策树剪枝的问题可以看做是对整个模型的损失函数求极小值。这里我们不妨假设已经生成的决策树 \\(T\\) 的叶子结点个数为 \\(|T|\\) ， \\(t\\) 为其中的一个叶子结点，最终分类到该叶子结点的样本的个数为 \\(N_t\\) 个，其中 \\(k\\) 类的样本点有 \\(N_{tk}\\) 个， \\(H_t(T)\\) 为叶子结点 \\(t\\) 上的经验熵， \\(\\alpha \\geqslant 0\\) 为参数，那么模型的损失函数即为： $$C_{\\alpha}(T) = \\sum_{t=1}&#94;{|T|}N_tH_t(T)+\\alpha |T|$$ 则经验熵为： $$H_t(T) = -\\sum_{k}&#94;{}\\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{N_t}$$ 将 \\(H_t(T)\\) 带入上式，可得： $$C_{\\alpha}(T) = \\sum_{t=1}&#94;{|T|}\\sum_{k=1}&#94;{K}N_{tk}log\\frac{N_{tk}}{N_t}+\\alpha |T|$$ 上式中的第一项表示模型对训练数据的预测误差，也就是拟合程度， \\(\\alpha\\) 参数用来控制两者之间的影响大小， \\(\\alpha\\) 值较大则会模型简单，反之亦然。 所以我们剪枝的操作，说白了就是在已经固定 \\(\\alpha\\) 的前提下，选择损失函数最小的模型。可以想见的情况是，如果 \\(\\alpha\\) 已经固定，此时如果子树越大，往往与训练数据就会拟合得越好，但是模型就会更加复杂，反之亦然，而损失函数代表的恰恰就是两者之间的 tradeoff。 那么我们说到的损失函数的极小化问题应该如何解决呢？很显然，我们可以等价地将这个问题转化为正则化的极大似然估计。 也就是说，这种使用损失函数极小化的思路进行剪枝的操作就等价于使用正则化的极大似然估计进行模型选择。 基本的剪枝流程如下： 输入 ：完全的过拟合的决策树模型 \\(T\\) ，参数 \\(\\alpha\\) ； 输出 ：完成剪枝操作后的子树 \\(T_\\alpha\\) ； 步骤 ： 计算每个结点的经验熵； 递归地从树的叶子结点向上回缩，这里我们不妨假设一组叶子结点回缩到其父节点之前与之后的整体决策树模型分别为 \\(T_{before}\\) 和 \\(T_{after}\\) ，以及它们对应的损失函数分别为 \\(C_\\alpha(T_{before})\\) 与 \\(C_\\alpha(T_{after})\\) ，此时，如果有： $$C_\\alpha(T_{before}) \\geqslant C_\\alpha(T_{after})$$ 则进行剪枝操作，也即将其父节点变成新的叶子结点，原叶子结点丢弃。 继续执行步骤 2 直到停止，最终返回损失函数最小的子树 \\(T_\\alpha\\) ，算法结束。 上图为剪枝操作的示意图，将部分结点裁剪后的决策树模型虽然变得更加简单，却拥有了更强的泛化能力，很好地应对训练数据过拟合的问题。 虽然这样的决策树模型已经拥有较好的分类性能，但是在实践中我们发现那些被错误分类的样本，却没有得到应有的重视，这样的分类器的适应能力并不好，以及它是不具备自我修正能力的。 「 将那些被错误归类的样本加大权重，放回分类器继续学习，直到它们也被正确地分类 」，这是一个可以尝试的继续优化的思路。 AdaBoost 集成 以上的单棵决策树模型求解思路属于传统的机器学习方法，在一个由各种可能的函数构成空间中寻找一个最接近实际分类函数 \\(f(x)\\) 的分类器 \\(h(x)\\) 。而集成学习的思想是在对新的样本进行分类的时候，把若干个分类器结合起来，将它们的结果进行某种组合来决定最终的分类。 在集成学习中，基本分类器之间的协作关系常见的有 Bagging 思路和 Boosting 思路。Bagging 思路是最简单最直观的一种，其主要思想是对训练集 有放回地 抽取训练样本，从而为每一个弱分类器构造出一个与训练集 \\(D\\) 大小相同，但样本却不同的训练集 \\(\\bar{D}\\) ，从而训练出不同的弱分类器。常见的随机森林算法就是它最为典型的应用之一。 Bagging 思想如下： 输入 ：弱分类器： \\(L\\) ；弱分类器个数： \\(N\\) ；数据集： \\(D\\) ； 输出 ： \\(h_f(x) = \\underset{y\\in Y}{argmax}\\sum_{i=1}&#94;{N}h_i(x)=y\\) 步骤 ： 从数据集 \\(D\\) 中抽样得到新的数据集 \\(\\bar{D}\\) ； 将 \\(\\bar{D}\\) 放入弱分类器中，即 \\(h_i=L(\\bar{D})\\) ； 循环 \\(N\\) 次，算法结束。 相比于 Bagging，Boosting 思想则在计算上更符合情理，实践中也更符合我们的需求。 Boosting 思想是对那些被分类错误的训练样本进行加强学习，具体来说，首先给每一个训练样本赋予相同的权重，然后训练第一个弱分类器并用它来对训练集进行验证，对于那些被分类错误的样本加大权重，也即把训练集合中的所有样本权重值进行更新，然后用这些带有新的权重的训练集去训练第二个弱分类器，不断重复这个过程直到全部分类正确或者强制停止。 那么 Boosting 思想中，最为常用的莫过于 AdaBoost 方法，在 Boosting 阶段之后，会将得到的若干弱分类器进行合理的线性组合，也即加大正确率高的弱分类器的权值，减少正确率低的弱分类器的权值。 对于 AdaBoost 的流程，我们不妨先用一张图来认识： 下面是 AdaBoost 算法的具体执行步骤： 输入 ：数据集： \\(D={(x_1,y_1), (x_2,y_2),…,(x_N,y_N)}, y_i={-1,1}\\) ，弱分类器 \\(L\\) ，弱分类器个数 \\(M\\) ； 输出 ：最终分类器模型 \\(G(x)\\) ； 步骤 ： 首先需要对训练集合中样本的权值进行初始化操作： $$W_1 = (w_{11},w_{12},…,w_{1i},w_{1N}), w_{1i}=\\frac{1}{N}, i=1,2,…,N$$ 训练集 \\(D\\) 的样本权值更新为 \\(W_m\\) （初始化时 \\(m=1\\) ，即保持不变），放入弱分类器进行学习，得到模型 \\(G_m(x)\\) ； 计算 \\(G_m(x)\\) 在 \\(D\\) 上的错误率： $$e_m = P(G_m(x_i)\\neq y_i)=\\sum_{i=1}&#94;{N}w_{mi}I(G_m(x_i)\\neq y_i)$$ 计算 \\(G_m(x)\\) 的系数为： $$\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m}$$ 依据错误情况更新样本的权值空间为： $$W_{m+1} = (w_{m+1,1},w_{m+1,2},…,w_{m+1,i},…,w_{m+1,N})$$ 其中依据为： $$w_{m+1,i}=\\frac{w_{mi}}{Z_m}e&#94;{-\\alpha_m y_iG_m(x_i)}, i = 1,2,…,N$$ 其中 \\(Z_m\\) 为规范化因子： $$Z_m=\\sum_{i=1}&#94;{N}w_{mi}e&#94;{-\\alpha_my_iG_m(x_i)}$$ 重复步骤2~5直到循环结束，得到最终分类器为： $$G(x)=sign(f(x))=sign(\\sum_{m=1}&#94;{M}\\alpha_mG_m(x))$$ 那么最终我们的分类模型就是 AdaBoost 的输出 \\(G(x)\\) ，它将用户的社交行为当做输入，输出为用户是否为 spammer 的判断。 总结 通过大量的数据调研，我们发现 spammer 在站内的行为与内容两个方面上都有区别于正常用户，两者既相互独立又相辅相成，据此我们的反垃圾系统从用户行为和内容分别进行判断。 对于用户行为，我们采用C4.5决策树进行学习和分类，为了使模型具备更强的泛化能力，我们为它设计了剪枝操作，在最后我们使用 AdaBoost 提升算法进一步使模型具备一定的自我修正能力。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"算法团队","title":"决策树算法在果壳网反垃圾系统中的实践"}]}